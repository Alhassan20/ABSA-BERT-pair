{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPJ4+qNaykIpL3zPrct3aFa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alhassan20/ABSA-BERT-pair/blob/master/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8p9G7OpZpwC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ad32c3f-3bea-44b8-af4d-b73662acddfd"
      },
      "source": [
        "import os\n",
        "os.system('/content/drive/My Drive/My Data/generate/make.sh')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32512"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udvvC1Rji-V3",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0A5uUE7iuqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#import subprocess\n",
        "#subprocess.call('/content/drive/My Drive/My Data/generate/make.sh')"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa42v5xyasuD",
        "colab_type": "text"
      },
      "source": [
        "# tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T36GcFdEatIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "\n",
        "# Reference: https://github.com/huggingface/pytorch-pretrained-BERT\n",
        "\n",
        "\"\"\"Tokenization classes.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import collections\n",
        "import unicodedata\n",
        "\n",
        "import six\n",
        "\n",
        "\n",
        "def convert_to_unicode(text):\n",
        "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
        "    if six.PY3:\n",
        "        if isinstance(text, str):\n",
        "            return text\n",
        "        elif isinstance(text, bytes):\n",
        "            return text.decode(\"utf-8\", \"ignore\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "    elif six.PY2:\n",
        "        if isinstance(text, str):\n",
        "            return text.decode(\"utf-8\", \"ignore\")\n",
        "        elif isinstance(text, unicode):\n",
        "            return text\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "    else:\n",
        "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def printable_text(text):\n",
        "    \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n",
        "\n",
        "    # These functions want `str` for both Python2 and Python3, but in one case\n",
        "    # it's a Unicode string and in the other it's a byte string.\n",
        "    if six.PY3:\n",
        "        if isinstance(text, str):\n",
        "            return text\n",
        "        elif isinstance(text, bytes):\n",
        "            return text.decode(\"utf-8\", \"ignore\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "    elif six.PY2:\n",
        "        if isinstance(text, str):\n",
        "            return text\n",
        "        elif isinstance(text, unicode):\n",
        "            return text.encode(\"utf-8\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "    else:\n",
        "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "    vocab = collections.OrderedDict()\n",
        "    index = 0\n",
        "    with open(vocab_file, \"r\") as reader:\n",
        "        while True:\n",
        "            token = convert_to_unicode(reader.readline())\n",
        "            if not token:\n",
        "                break\n",
        "            token = token.strip()\n",
        "            vocab[token] = index\n",
        "            index += 1\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def convert_tokens_to_ids(vocab, tokens):\n",
        "    \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
        "    ids = []\n",
        "    for token in tokens:\n",
        "        ids.append(vocab[token])\n",
        "    return ids\n",
        "\n",
        "\n",
        "def whitespace_tokenize(text):\n",
        "    \"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "\n",
        "class FullTokenizer(object):\n",
        "    \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_file, do_lower_case=True):\n",
        "        self.vocab = load_vocab(vocab_file)\n",
        "        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
        "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        split_tokens = []\n",
        "        for token in self.basic_tokenizer.tokenize(text):\n",
        "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "                split_tokens.append(sub_token)\n",
        "\n",
        "        return split_tokens\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        return convert_tokens_to_ids(self.vocab, tokens)\n",
        "\n",
        "\n",
        "class BasicTokenizer(object):\n",
        "    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
        "\n",
        "    def __init__(self, do_lower_case=True):\n",
        "        \"\"\"Constructs a BasicTokenizer.\n",
        "        Args:\n",
        "          do_lower_case: Whether to lower case the input.\n",
        "        \"\"\"\n",
        "        self.do_lower_case = do_lower_case\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenizes a piece of text.\"\"\"\n",
        "        text = convert_to_unicode(text)\n",
        "        text = self._clean_text(text)\n",
        "        orig_tokens = whitespace_tokenize(text)\n",
        "        split_tokens = []\n",
        "        for token in orig_tokens:\n",
        "            if self.do_lower_case:\n",
        "                token = token.lower()\n",
        "                token = self._run_strip_accents(token)\n",
        "            split_tokens.extend(self._run_split_on_punc(token))\n",
        "\n",
        "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
        "        return output_tokens\n",
        "\n",
        "    def _run_strip_accents(self, text):\n",
        "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
        "        text = unicodedata.normalize(\"NFD\", text)\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cat = unicodedata.category(char)\n",
        "            if cat == \"Mn\":\n",
        "                continue\n",
        "            output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "    def _run_split_on_punc(self, text):\n",
        "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
        "        chars = list(text)\n",
        "        i = 0\n",
        "        start_new_word = True\n",
        "        output = []\n",
        "        while i < len(chars):\n",
        "            char = chars[i]\n",
        "            if _is_punctuation(char):\n",
        "                output.append([char])\n",
        "                start_new_word = True\n",
        "            else:\n",
        "                if start_new_word:\n",
        "                    output.append([])\n",
        "                start_new_word = False\n",
        "                output[-1].append(char)\n",
        "            i += 1\n",
        "\n",
        "        return [\"\".join(x) for x in output]\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cp = ord(char)\n",
        "            if cp == 0 or cp == 0xfffd or _is_control(char):\n",
        "                continue\n",
        "            if _is_whitespace(char):\n",
        "                output.append(\" \")\n",
        "            else:\n",
        "                output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "\n",
        "class WordpieceTokenizer(object):\n",
        "    \"\"\"Runs WordPiece tokenization.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=100):\n",
        "        self.vocab = vocab\n",
        "        self.unk_token = unk_token\n",
        "        self.max_input_chars_per_word = max_input_chars_per_word\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenizes a piece of text into its word pieces.\n",
        "        This uses a greedy longest-match-first algorithm to perform tokenization\n",
        "        using the given vocabulary.\n",
        "        For example:\n",
        "          input = \"unaffable\"\n",
        "          output = [\"un\", \"##aff\", \"##able\"]\n",
        "        Args:\n",
        "          text: A single token or whitespace separated tokens. This should have\n",
        "            already been passed through `BasicTokenizer.\n",
        "        Returns:\n",
        "          A list of wordpiece tokens.\n",
        "        \"\"\"\n",
        "\n",
        "        text = convert_to_unicode(text)\n",
        "\n",
        "        output_tokens = []\n",
        "        for token in whitespace_tokenize(text):\n",
        "            chars = list(token)\n",
        "            if len(chars) > self.max_input_chars_per_word:\n",
        "                output_tokens.append(self.unk_token)\n",
        "                continue\n",
        "\n",
        "            is_bad = False\n",
        "            start = 0\n",
        "            sub_tokens = []\n",
        "            while start < len(chars):\n",
        "                end = len(chars)\n",
        "                cur_substr = None\n",
        "                while start < end:\n",
        "                    substr = \"\".join(chars[start:end])\n",
        "                    if start > 0:\n",
        "                        substr = \"##\" + substr\n",
        "                    if substr in self.vocab:\n",
        "                        cur_substr = substr\n",
        "                        break\n",
        "                    end -= 1\n",
        "                if cur_substr is None:\n",
        "                    is_bad = True\n",
        "                    break\n",
        "                sub_tokens.append(cur_substr)\n",
        "                start = end\n",
        "\n",
        "            if is_bad:\n",
        "                output_tokens.append(self.unk_token)\n",
        "            else:\n",
        "                output_tokens.extend(sub_tokens)\n",
        "        return output_tokens\n",
        "\n",
        "\n",
        "def _is_whitespace(char):\n",
        "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
        "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
        "    # as whitespace since they are generally considered as such.\n",
        "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat == \"Zs\":\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_control(char):\n",
        "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
        "    # These are technically control characters but we count them as whitespace\n",
        "    # characters.\n",
        "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return False\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"C\"):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_punctuation(char):\n",
        "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
        "    cp = ord(char)\n",
        "    # We treat all non-letter/number ASCII as punctuation.\n",
        "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
        "    # Punctuation class but we treat them as punctuation anyways, for\n",
        "    # consistency.\n",
        "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
        "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"P\"):\n",
        "        return True\n",
        "    return False"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8QCfjtFat-y",
        "colab_type": "text"
      },
      "source": [
        "# modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBrjUgcwZ4gO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "\n",
        "# Reference: https://github.com/huggingface/pytorch-pretrained-BERT\n",
        "\n",
        "\"\"\"PyTorch BERT model.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "\n",
        "import six\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"Implementation of the gelu activation function.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "class BertConfig(object):\n",
        "    \"\"\"Configuration class to store the configuration of a `BertModel`.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                vocab_size,\n",
        "                hidden_size=768,\n",
        "                num_hidden_layers=12,\n",
        "                num_attention_heads=12,\n",
        "                intermediate_size=3072,\n",
        "                hidden_act=\"gelu\",\n",
        "                hidden_dropout_prob=0.1,\n",
        "                attention_probs_dropout_prob=0.1,\n",
        "                max_position_embeddings=512,\n",
        "                type_vocab_size=16,\n",
        "                initializer_range=0.02):\n",
        "        \"\"\"Constructs BertConfig.\n",
        "        Args:\n",
        "            vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n",
        "            hidden_size: Size of the encoder layers and the pooler layer.\n",
        "            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n",
        "            num_attention_heads: Number of attention heads for each attention layer in\n",
        "                the Transformer encoder.\n",
        "            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
        "                layer in the Transformer encoder.\n",
        "            hidden_act: The non-linear activation function (function or string) in the\n",
        "                encoder and pooler.\n",
        "            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
        "                layers in the embeddings, encoder, and pooler.\n",
        "            attention_probs_dropout_prob: The dropout ratio for the attention\n",
        "                probabilities.\n",
        "            max_position_embeddings: The maximum sequence length that this model might\n",
        "                ever be used with. Typically set this to something large just in case\n",
        "                (e.g., 512 or 1024 or 2048).\n",
        "            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
        "                `BertModel`.\n",
        "            initializer_range: The sttdev of the truncated_normal_initializer for\n",
        "                initializing all weight matrices.\n",
        "        \"\"\"\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.hidden_act = hidden_act\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.type_vocab_size = type_vocab_size\n",
        "        self.initializer_range = initializer_range\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, json_object):\n",
        "        \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n",
        "        config = BertConfig(vocab_size=None)\n",
        "        for (key, value) in six.iteritems(json_object):\n",
        "            config.__dict__[key] = value\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_json_file(cls, json_file):\n",
        "        \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
        "        with open(json_file, \"r\") as reader:\n",
        "            text = reader.read()\n",
        "        return cls.from_dict(json.loads(text))\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "class BERTLayerNorm(nn.Module):\n",
        "    def __init__(self, config, variance_epsilon=1e-12):\n",
        "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
        "        \"\"\"\n",
        "        super(BERTLayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(config.hidden_size))\n",
        "        self.beta = nn.Parameter(torch.zeros(config.hidden_size))\n",
        "        self.variance_epsilon = variance_epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = x.mean(-1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "        return self.gamma * x + self.beta\n",
        "\n",
        "class BERTEmbeddings(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTEmbeddings, self).__init__()\n",
        "        \"\"\"Construct the embedding module from word, position and token_type embeddings.\n",
        "        \"\"\"\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = BERTLayerNorm(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None):\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        words_embeddings = self.word_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class BERTSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTSelfAttention, self).__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        return context_layer\n",
        "\n",
        "\n",
        "class BERTSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTSelfOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = BERTLayerNorm(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BERTAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTAttention, self).__init__()\n",
        "        self.self = BERTSelfAttention(config)\n",
        "        self.output = BERTSelfOutput(config)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask):\n",
        "        self_output = self.self(input_tensor, attention_mask)\n",
        "        attention_output = self.output(self_output, input_tensor)\n",
        "        return attention_output\n",
        "\n",
        "\n",
        "class BERTIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTIntermediate, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.intermediate_act_fn = gelu\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BERTOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = BERTLayerNorm(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BERTLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTLayer, self).__init__()\n",
        "        self.attention = BERTAttention(config)\n",
        "        self.intermediate = BERTIntermediate(config)\n",
        "        self.output = BERTOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        attention_output = self.attention(hidden_states, attention_mask)\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "class BERTEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTEncoder, self).__init__()\n",
        "        layer = BERTLayer(config)\n",
        "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])    \n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        all_encoder_layers = []\n",
        "        for layer_module in self.layer:\n",
        "            hidden_states = layer_module(hidden_states, attention_mask)\n",
        "            all_encoder_layers.append(hidden_states)\n",
        "        return all_encoder_layers\n",
        "\n",
        "\n",
        "class BERTPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTPooler, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        #return first_token_tensor\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class BertModel(nn.Module):\n",
        "    \"\"\"BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n",
        "    config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n",
        "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "    model = modeling.BertModel(config=config)\n",
        "    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config: BertConfig):\n",
        "        \"\"\"Constructor for BertModel.\n",
        "        Args:\n",
        "            config: `BertConfig` instance.\n",
        "        \"\"\"\n",
        "        super(BertModel, self).__init__()\n",
        "        self.embeddings = BERTEmbeddings(config)\n",
        "        self.encoder = BERTEncoder(config)\n",
        "        self.pooler = BERTPooler(config)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\n",
        "        # Sizes are [batch_size, 1, 1, from_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, to_seq_length, from_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        extended_attention_mask = extended_attention_mask.float()\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
        "        all_encoder_layers = self.encoder(embedding_output, extended_attention_mask)\n",
        "        sequence_output = all_encoder_layers[-1]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        return all_encoder_layers, pooled_output\n",
        "\n",
        "class BertForSequenceClassification(nn.Module):\n",
        "    \"\"\"BERT model for classification.\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the pooled output.\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n",
        "    config = BertConfig(vocab_size=32000, hidden_size=512,\n",
        "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "    num_labels = 2\n",
        "    model = BertForSequenceClassification(config, num_labels)\n",
        "    logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config, num_labels):\n",
        "        super(BertForSequenceClassification, self).__init__()\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "\n",
        "        def init_weights(module):\n",
        "            if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "                # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "                # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "            elif isinstance(module, BERTLayerNorm):\n",
        "                module.beta.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "                module.gamma.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "            if isinstance(module, nn.Linear):\n",
        "                module.bias.data.zero_()\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids, attention_mask, labels=None):\n",
        "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "            return loss, logits\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "\n",
        "class BertForQuestionAnswering(nn.Module):\n",
        "    \"\"\"BERT model for Question Answering (span extraction).\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the sequence output that computes start_logits and end_logits\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n",
        "    config = BertConfig(vocab_size=32000, hidden_size=512,\n",
        "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "    model = BertForQuestionAnswering(config)\n",
        "    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForQuestionAnswering, self).__init__()\n",
        "        self.bert = BertModel(config)\n",
        "        # TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version\n",
        "        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "        def init_weights(module):\n",
        "            if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "                # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "                # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "            elif isinstance(module, BERTLayerNorm):\n",
        "                module.beta.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "                module.gamma.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "            if isinstance(module, nn.Linear):\n",
        "                module.bias.data.zero_()\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids, attention_mask, start_positions=None, end_positions=None):\n",
        "        all_encoder_layers, _ = self.bert(input_ids, token_type_ids, attention_mask)\n",
        "        sequence_output = all_encoder_layers[-1]\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension - if not this is a no-op\n",
        "            start_positions = start_positions.squeeze(-1)\n",
        "            end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions.clamp_(0, ignored_index)\n",
        "            end_positions.clamp_(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "            return total_loss\n",
        "        else:\n",
        "            return start_logits, end_logits"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcdawMGhbwXe",
        "colab_type": "text"
      },
      "source": [
        "# convert_tf_checkpoint_to_pytorch (it's ***Done***)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBR06_uMbwvl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f88dd481-8e56-458e-da71-022d80906bb7"
      },
      "source": [
        "# coding=utf-8\n",
        "\n",
        "# Reference: https://github.com/huggingface/pytorch-pretrained-BERT\n",
        "\n",
        "\"\"\"Convert BERT checkpoint.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import argparse\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import tensorflow as tf\n",
        "#from modeling import BertConfig, BertModel\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "## Required parameters\n",
        "parser.add_argument(\"--tf_checkpoint_path\", # uncased_L-12_H-768_A-12/bert_model.ckpt.index\n",
        "                    default = None,\n",
        "                    type = str,\n",
        "                    required = True,\n",
        "                    help = \"Path the TensorFlow checkpoint path.\")\n",
        "parser.add_argument(\"--bert_config_file\",  # uncased_L-12_H-768_A-12/bert_config.json\n",
        "                    default = None,\n",
        "                    type = str,\n",
        "                    required = True,\n",
        "                    help = \"The config json file corresponding to the pre-trained BERT model. \\n\"\n",
        "                        \"This specifies the model architecture.\")\n",
        "parser.add_argument(\"--pytorch_dump_path\",\n",
        "                    default = None,\n",
        "                    type = str,\n",
        "                    required = True,\n",
        "                    help = \"Path to the output PyTorch model.\")\n",
        "\n",
        "args = parser.parse_args(args=['--tf_checkpoint_path','/content/drive/My Drive/My Data/uncased_L-12_H-768_A-12/bert_model.ckpt.index','--bert_config_file','/content/drive/My Drive/My Data/uncased_L-12_H-768_A-12/bert_config.json','--pytorch_dump_path','/content/drive/My Drive/My Data/uncased_L-12_H-768_A-12/pytorch_model.bin'])\n",
        "\n",
        "def convert():\n",
        "    # Initialise PyTorch model\n",
        "    config = BertConfig.from_json_file(args.bert_config_file)\n",
        "    model = BertModel(config)\n",
        "\n",
        "    # Load weights from TF model\n",
        "    path = args.tf_checkpoint_path\n",
        "    print(\"Converting TensorFlow checkpoint from {}\".format(path))\n",
        "\n",
        "    init_vars = tf.train.list_variables(path)\n",
        "    names = []\n",
        "    arrays = []\n",
        "    for name, shape in init_vars:\n",
        "        print(\"Loading {} with shape {}\".format(name, shape))\n",
        "        array = tf.train.load_variable(path, name)\n",
        "        print(\"Numpy array shape {}\".format(array.shape))\n",
        "        names.append(name)\n",
        "        arrays.append(array)\n",
        "\n",
        "    for name, array in zip(names, arrays):\n",
        "        name = name[5:]  # skip \"bert/\"\n",
        "        print(\"Loading {}\".format(name))\n",
        "        name = name.split('/')\n",
        "        if any(n in [\"adam_v\", \"adam_m\",\"l_step\"] for n in name):\n",
        "            print(\"Skipping {}\".format(\"/\".join(name)))\n",
        "            continue\n",
        "        if name[0] in ['redictions', 'eq_relationship']:\n",
        "            print(\"Skipping\")\n",
        "            continue\n",
        "        pointer = model\n",
        "        for m_name in name:\n",
        "            if re.fullmatch(r'[A-Za-z]+_\\d+', m_name):\n",
        "                l = re.split(r'_(\\d+)', m_name)\n",
        "            else:\n",
        "                l = [m_name]\n",
        "            if l[0] == 'kernel':\n",
        "                pointer = getattr(pointer, 'weight')\n",
        "            else:\n",
        "                pointer = getattr(pointer, l[0])\n",
        "            if len(l) >= 2:\n",
        "                num = int(l[1])\n",
        "                pointer = pointer[num]\n",
        "        if m_name[-11:] == '_embeddings':\n",
        "            pointer = getattr(pointer, 'weight')\n",
        "        elif m_name == 'kernel':\n",
        "            array = np.transpose(array)\n",
        "        try:\n",
        "            assert pointer.shape == array.shape\n",
        "        except AssertionError as e:\n",
        "            e.args += (pointer.shape, array.shape)\n",
        "            raise\n",
        "        pointer.data = torch.from_numpy(array)\n",
        "\n",
        "    # Save pytorch-model\n",
        "    torch.save(model.state_dict(), args.pytorch_dump_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    convert()\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converting TensorFlow checkpoint from /content/drive/My Drive/My Data/uncased_L-12_H-768_A-12/bert_model.ckpt.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9tYL0omafXV",
        "colab_type": "text"
      },
      "source": [
        "# processor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoyjFxmsZ4ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "\n",
        "\"\"\"Processors for different tasks.\"\"\"\n",
        "\n",
        "import csv\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#import tokenization\n",
        "\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "    \n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the test set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "\n",
        "class Sentihood_single_Processor(DataProcessor):\n",
        "    \"\"\"Processor for the Sentihood data set.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        train_data = pd.read_csv(os.path.join(data_dir, \"train.tsv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(train_data, \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        dev_data = pd.read_csv(os.path.join(data_dir, \"dev.tsv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(dev_data, \"dev\")\n",
        "    \n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        test_data = pd.read_csv(os.path.join(data_dir, \"test.tsv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(test_data, \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return ['None', 'Positive', 'Negative']\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "          #  if i>50:break\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = convert_to_unicode(str(line[1]))\n",
        "            label = convert_to_unicode(str(line[2]))\n",
        "            if i%1000==0:\n",
        "                print(i)\n",
        "                print(\"guid=\",guid)\n",
        "                print(\"text_a=\",text_a)\n",
        "                print(\"label=\",label)\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "        return examples\n",
        "\n",
        "        \n",
        "class Sentihood_NLI_M_Processor(DataProcessor):\n",
        "    \"\"\"Processor for the Sentihood data set.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        train_data = pd.read_csv(os.path.join(data_dir, \"/content/drive/My Drive/My Data/data/sentihood/bert-pair/train_NLI_M.tsv\"),sep=\"\\t\").values\n",
        "        return self._create_examples(train_data, \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        dev_data = pd.read_csv(os.path.join(data_dir, \"dev_NLI_M.tsv\"),sep=\"\\t\").values\n",
        "        return self._create_examples(dev_data, \"dev\")\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        test_data = pd.read_csv(os.path.join(data_dir, \"test_NLI_M.tsv\"),sep=\"\\t\").values\n",
        "        return self._create_examples(test_data, \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return ['None', 'Positive', 'Negative']\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "          #  if i>50:break\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = convert_to_unicode(str(line[1]))\n",
        "            text_b = convert_to_unicode(str(line[2]))\n",
        "            label = convert_to_unicode(str(line[3]))\n",
        "            if i%1000==0:\n",
        "                print(i)\n",
        "                print(\"guid=\",guid)\n",
        "                print(\"text_a=\",text_a)\n",
        "                print(\"text_b=\",text_b)\n",
        "                print(\"label=\",label)\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "class Sentihood_QA_M_Processor(DataProcessor):\n",
        "    \"\"\"Processor for the Sentihood data set.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        train_data = pd.read_csv(os.path.join(data_dir, \"train_QA_M.tsv\"),sep=\"\\t\").values\n",
        "        return self._create_examples(train_data, \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        dev_data = pd.read_csv(os.path.join(data_dir, \"dev_QA_M.tsv\"),sep=\"\\t\").values\n",
        "        return self._create_examples(dev_data, \"dev\")\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        test_data = pd.read_csv(os.path.join(data_dir, \"test_QA_M.tsv\"),sep=\"\\t\").values\n",
        "        return self._create_examples(test_data, \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return ['None', 'Positive', 'Negative']\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "          #  if i>50:break\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = convert_to_unicode(str(line[1]))\n",
        "            text_b = convert_to_unicode(str(line[2]))\n",
        "            label = convert_to_unicode(str(line[3]))\n",
        "            if i%1000==0:\n",
        "                print(i)\n",
        "                print(\"guid=\",guid)\n",
        "                print(\"text_a=\",text_a)\n",
        "                print(\"text_b=\",text_b)\n",
        "                print(\"label=\",label)\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "class Sentihood_NLI_B_Processor(DataProcessor):\n",
        "    \"\"\"Processor for the Sentihood data set.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        train_data = pd.read_csv(os.path.join(data_dir, \"train_NLI_B.tsv\"),sep=\"\\t\").values\n",
        "        return self._create_examples(train_data, \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        dev_data = pd.read_csv(os.path.join(data_dir, \"dev_NLI_B.tsv\"),sep=\"\\t\").values\n",
        "        return self._create_examples(dev_data, \"dev\")\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        test_data = pd.read_csv(os.path.join(data_dir, \"test_NLI_B.tsv\"),sep=\"\\t\").values\n",
        "        return self._create_examples(test_data, \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return ['0', '1']\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "          #  if i>50:break\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = convert_to_unicode(str(line[2]))\n",
        "            text_b = convert_to_unicode(str(line[1]))\n",
        "            label = convert_to_unicode(str(line[3]))\n",
        "            if i%1000==0:\n",
        "                print(i)\n",
        "                print(\"guid=\",guid)\n",
        "                print(\"text_a=\",text_a)\n",
        "                print(\"text_b=\",text_b)\n",
        "                print(\"label=\",label)\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "class Sentihood_QA_B_Processor(DataProcessor):\n",
        "    \"\"\"Processor for the Sentihood data set.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        train_data = pd.read_csv(os.path.join(data_dir, \"train_QA_B.tsv\"),sep=\"\\t\").values\n",
        "        return self._create_examples(train_data, \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        dev_data = pd.read_csv(os.path.join(data_dir, \"dev_QA_B.tsv\"),sep=\"\\t\").values\n",
        "        return self._create_examples(dev_data, \"dev\")\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        test_data = pd.read_csv(os.path.join(data_dir, \"test_QA_B.tsv\"),sep=\"\\t\").values\n",
        "        return self._create_examples(test_data, \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return ['0', '1']\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "          #  if i>50:break\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = convert_to_unicode(str(line[2]))\n",
        "            text_b = convert_to_unicode(str(line[1]))\n",
        "            label = convert_to_unicode(str(line[3]))\n",
        "            if i%1000==0:\n",
        "                print(i)\n",
        "                print(\"guid=\",guid)\n",
        "                print(\"text_a=\",text_a)\n",
        "                print(\"text_b=\",text_b)\n",
        "                print(\"label=\",label)\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
        "        return examples\n",
        "\n",
        "        \n",
        "class Semeval_single_Processor(DataProcessor):\n",
        "    \"\"\"Processor for the Semeval 2014 data set.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        train_data = pd.read_csv(os.path.join(data_dir, \"train.csv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(train_data, \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        dev_data = pd.read_csv(os.path.join(data_dir, \"dev.csv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(dev_data, \"dev\")\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        test_data = pd.read_csv(os.path.join(data_dir, \"test.csv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(test_data, \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return ['positive', 'neutral', 'negative', 'conflict', 'none']\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "          #  if i>50:break\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = convert_to_unicode(str(line[3]))\n",
        "            label = convert_to_unicode(str(line[1]))\n",
        "            if i%1000==0:\n",
        "                print(i)\n",
        "                print(\"guid=\",guid)\n",
        "                print(\"text_a=\",text_a)\n",
        "                print(\"label=\",label)\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "class Semeval_NLI_M_Processor(DataProcessor):\n",
        "    \"\"\"Processor for the Semeval 2014 data set.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        train_data = pd.read_csv(os.path.join(data_dir, \"/content/drive/My Drive/My Data/data/semeval2014/bert-pair/train_NLI_M.csv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(train_data, \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        dev_data = pd.read_csv(os.path.join(data_dir, \"dev_NLI_M.csv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(dev_data, \"dev\")\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        test_data = pd.read_csv(os.path.join(data_dir, \"test_NLI_M.csv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(test_data, \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return ['positive', 'neutral', 'negative', 'conflict', 'none']\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "          #  if i>50:break\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = convert_to_unicode(str(line[3]))\n",
        "            text_b = convert_to_unicode(str(line[2]))\n",
        "            label = convert_to_unicode(str(line[1]))\n",
        "            if i%1000==0:\n",
        "                print(i)\n",
        "                print(\"guid=\",guid)\n",
        "                print(\"text_a=\",text_a)\n",
        "                print(\"label=\",label)\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "class Semeval_QA_M_Processor(DataProcessor):\n",
        "    \"\"\"Processor for the Semeval 2014 data set.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        train_data = pd.read_csv(os.path.join(data_dir, \"train_QA_M.csv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(train_data, \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        dev_data = pd.read_csv(os.path.join(data_dir, \"dev_QA_M.csv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(dev_data, \"dev\")\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        test_data = pd.read_csv(os.path.join(data_dir, \"test_QA_M.csv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(test_data, \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return ['positive', 'neutral', 'negative', 'conflict', 'none']\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "          #  if i>50:break\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = convert_to_unicode(str(line[3]))\n",
        "            text_b = convert_to_unicode(str(line[2]))\n",
        "            label = convert_to_unicode(str(line[1]))\n",
        "            if i%1000==0:\n",
        "                print(i)\n",
        "                print(\"guid=\",guid)\n",
        "                print(\"text_a=\",text_a)\n",
        "                print(\"label=\",label)\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "class Semeval_NLI_B_Processor(DataProcessor):\n",
        "    \"\"\"Processor for the Semeval 2014 data set.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        train_data = pd.read_csv(os.path.join(data_dir, \"train_NLI_B.csv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(train_data, \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        dev_data = pd.read_csv(os.path.join(data_dir, \"dev_NLI_B.csv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(dev_data, \"dev\")\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        test_data = pd.read_csv(os.path.join(data_dir, \"test_NLI_B.csv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(test_data, \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return ['0', '1']\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "          #  if i>50:break\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = convert_to_unicode(str(line[2]))\n",
        "            text_b = convert_to_unicode(str(line[3]))\n",
        "            label = convert_to_unicode(str(line[1]))\n",
        "            if i%1000==0:\n",
        "                print(i)\n",
        "                print(\"guid=\",guid)\n",
        "                print(\"text_a=\",text_a)\n",
        "                print(\"label=\",label)\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "class Semeval_QA_B_Processor(DataProcessor):\n",
        "    \"\"\"Processor for the Semeval 2014 data set.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        train_data = pd.read_csv(os.path.join(data_dir, \"train_QA_B.csv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(train_data, \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        dev_data = pd.read_csv(os.path.join(data_dir, \"dev_QA_B.csv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(dev_data, \"dev\")\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        test_data = pd.read_csv(os.path.join(data_dir, \"test_QA_B.csv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(test_data, \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return ['0', '1']\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "          #  if i>50:break\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = convert_to_unicode(str(line[2]))\n",
        "            text_b = convert_to_unicode(str(line[3]))\n",
        "            label = convert_to_unicode(str(line[1]))\n",
        "            if i%1000==0:\n",
        "                print(i)\n",
        "                print(\"guid=\",guid)\n",
        "                print(\"text_a=\",text_a)\n",
        "                print(\"label=\",label)\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
        "        return examples"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40a1ObNOaPz0",
        "colab_type": "text"
      },
      "source": [
        "# optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DsZb-rTZ4wl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "\n",
        "# Reference: https://github.com/huggingface/pytorch-pretrained-BERT\n",
        "\n",
        "\"\"\"PyTorch optimization for BERT model.\"\"\"\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "\n",
        "def warmup_cosine(x, warmup=0.002):\n",
        "    if x < warmup:\n",
        "        return x/warmup\n",
        "    return 0.5 * (1.0 + torch.cos(math.pi * x))\n",
        "\n",
        "def warmup_constant(x, warmup=0.002):\n",
        "    if x < warmup:\n",
        "        return x/warmup\n",
        "    return 1.0\n",
        "\n",
        "def warmup_linear(x, warmup=0.002):\n",
        "    if x < warmup:\n",
        "        return x/warmup\n",
        "    return 1.0 - x\n",
        "\n",
        "SCHEDULES = {\n",
        "    'warmup_cosine':warmup_cosine,\n",
        "    'warmup_constant':warmup_constant,\n",
        "    'warmup_linear':warmup_linear,\n",
        "}\n",
        "\n",
        "\n",
        "class BERTAdam(Optimizer):\n",
        "    \"\"\"Implements BERT version of Adam algorithm with weight decay fix (and no ).\n",
        "    Params:\n",
        "        lr: learning rate\n",
        "        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n",
        "        t_total: total number of training steps for the learning\n",
        "            rate schedule, -1  means constant learning rate. Default: -1\n",
        "        schedule: schedule to use for the warmup (see above). Default: 'warmup_linear'\n",
        "        b1: Adams b1. Default: 0.9\n",
        "        b2: Adams b2. Default: 0.999\n",
        "        e: Adams epsilon. Default: 1e-6\n",
        "        weight_decay_rate: Weight decay. Default: 0.01\n",
        "        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr, warmup=-1, t_total=-1, schedule='warmup_linear',\n",
        "                 b1=0.9, b2=0.999, e=1e-6, weight_decay_rate=0.01,\n",
        "                 max_grad_norm=1.0):\n",
        "        if not lr >= 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
        "        if schedule not in SCHEDULES:\n",
        "            raise ValueError(\"Invalid schedule parameter: {}\".format(schedule))\n",
        "        if not 0.0 <= warmup < 1.0 and not warmup == -1:\n",
        "            raise ValueError(\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\".format(warmup))\n",
        "        if not 0.0 <= b1 < 1.0:\n",
        "            raise ValueError(\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\".format(b1))\n",
        "        if not 0.0 <= b2 < 1.0:\n",
        "            raise ValueError(\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\".format(b2))\n",
        "        if not e >= 0.0:\n",
        "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(e))\n",
        "        defaults = dict(lr=lr, schedule=schedule, warmup=warmup, t_total=t_total,\n",
        "                        b1=b1, b2=b2, e=e, weight_decay_rate=weight_decay_rate,\n",
        "                        max_grad_norm=max_grad_norm)\n",
        "        super(BERTAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def get_lr(self):\n",
        "        lr = []\n",
        "        print(\"l_total=\",len(self.param_groups))\n",
        "        for group in self.param_groups:\n",
        "            print(\"l_p=\",len(group['params']))\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    return [0]\n",
        "                if group['t_total'] != -1:\n",
        "                    schedule_fct = SCHEDULES[group['schedule']]\n",
        "                    lr_scheduled = group['lr'] * schedule_fct(state['step']/group['t_total'], group['warmup'])\n",
        "                else:\n",
        "                    lr_scheduled = group['lr']\n",
        "                lr.append(lr_scheduled)\n",
        "        return lr\n",
        "\n",
        "    def to(self, device):\n",
        "        \"\"\" Move the optimizer state to a specified device\"\"\"\n",
        "        for state in self.state.values():\n",
        "            state['exp_avg'].to(device)\n",
        "            state['exp_avg_sq'].to(device)\n",
        "\n",
        "    def initialize_step(self, initial_step):\n",
        "        \"\"\"Initialize state with a defined step (but we don't have stored averaged).\n",
        "        Arguments:\n",
        "            initial_step (int): Initial step number.\n",
        "        \"\"\"\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                # State initialization\n",
        "                state['step'] = initial_step\n",
        "                # Exponential moving average of gradient values\n",
        "                state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                # Exponential moving average of squared gradient values\n",
        "                state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['next_m'] = torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['next_v'] = torch.zeros_like(p.data)\n",
        "\n",
        "                next_m, next_v = state['next_m'], state['next_v']\n",
        "                beta1, beta2 = group['b1'], group['b2']\n",
        "\n",
        "                # Add grad clipping\n",
        "                if group['max_grad_norm'] > 0:\n",
        "                    clip_grad_norm_(p, group['max_grad_norm'])\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                # In-place operations to update the averages at the same time\n",
        "                next_m.mul_(beta1).add_(1 - beta1, grad)\n",
        "                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                update = next_m / (next_v.sqrt() + group['e'])\n",
        "\n",
        "                # Just adding the square of the weights to the loss function is *not*\n",
        "                # the correct way of using L2 regularization/weight decay with Adam,\n",
        "                # since that will interact with the m and v parameters in strange ways.\n",
        "                #\n",
        "                # Instead we want ot decay the weights in a manner that doesn't interact\n",
        "                # with the m/v parameters. This is equivalent to adding the square\n",
        "                # of the weights to the loss with plain (non-momentum) SGD.\n",
        "                if group['weight_decay_rate'] > 0.0:\n",
        "                    update += group['weight_decay_rate'] * p.data\n",
        "\n",
        "                if group['t_total'] != -1:\n",
        "                    schedule_fct = SCHEDULES[group['schedule']]\n",
        "                    lr_scheduled = group['lr'] * schedule_fct(state['step']/group['t_total'], group['warmup'])\n",
        "                else:\n",
        "                    lr_scheduled = group['lr']\n",
        "\n",
        "                update_with_lr = lr_scheduled * update\n",
        "                p.data.add_(-update_with_lr)\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n",
        "                # bias_correction1 = 1 - beta1 ** state['step']\n",
        "                # bias_correction2 = 1 - beta2 ** state['step']\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHkwd1QYZ9Lm",
        "colab_type": "text"
      },
      "source": [
        "# run_classifier_TABSA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O61eH0onZ11y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "8aadf0f9-d3a9-4b91-e7f3-8babd82ebaf3"
      },
      "source": [
        "\"\"\"BERT finetuning runner.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import argparse\n",
        "import collections\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "\"\"\"\n",
        "import tokenization\n",
        "from modeling import BertConfig, BertForSequenceClassification\n",
        "from optimization import BERTAdam\n",
        "from processor import (Semeval_NLI_B_Processor, Semeval_NLI_M_Processor,\n",
        "                       Semeval_QA_B_Processor, Semeval_QA_M_Processor,\n",
        "                       Semeval_single_Processor, Sentihood_NLI_B_Processor,\n",
        "                       Sentihood_NLI_M_Processor, Sentihood_QA_B_Processor,\n",
        "                       Sentihood_QA_M_Processor, Sentihood_single_Processor)\n",
        "\"\"\"\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    label_map = {}\n",
        "    for (i, label) in enumerate(label_list):\n",
        "        label_map[label] = i\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(tqdm(examples)):\n",
        "        tokens_a = tokenizer.tokenize(example.text_a)\n",
        "\n",
        "        tokens_b = None\n",
        "        if example.text_b:\n",
        "            tokens_b = tokenizer.tokenize(example.text_b)\n",
        "\n",
        "        if tokens_b:\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > max_seq_length - 2:\n",
        "                tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
        "\n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids: 0   0   0   0  0     0 0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        tokens = []\n",
        "        segment_ids = []\n",
        "        tokens.append(\"[CLS]\")\n",
        "        segment_ids.append(0)\n",
        "        for token in tokens_a:\n",
        "            tokens.append(token)\n",
        "            segment_ids.append(0)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        segment_ids.append(0)\n",
        "\n",
        "        if tokens_b:\n",
        "            for token in tokens_b:\n",
        "                tokens.append(token)\n",
        "                segment_ids.append(1)\n",
        "            tokens.append(\"[SEP]\")\n",
        "            segment_ids.append(1)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        while len(input_ids) < max_seq_length:\n",
        "            input_ids.append(0)\n",
        "            input_mask.append(0)\n",
        "            segment_ids.append(0)\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        label_id = label_map[example.label]\n",
        "\n",
        "        features.append(\n",
        "                InputFeatures(\n",
        "                        input_ids=input_ids,\n",
        "                        input_mask=input_mask,\n",
        "                        segment_ids=segment_ids,\n",
        "                        label_id=label_id))\n",
        "    return features\n",
        "\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    ## Required parameters\n",
        "    parser.add_argument(\"--task_name\",\n",
        "                        default=None,\n",
        "                        type=str,\n",
        "                        required=True,\n",
        "                        choices=[\"sentihood_single\", \"sentihood_NLI_M\", \"sentihood_QA_M\", \\\n",
        "                                \"sentihood_NLI_B\", \"sentihood_QA_B\", \"semeval_single\", \\\n",
        "                                \"semeval_NLI_M\", \"semeval_QA_M\", \"semeval_NLI_B\", \"semeval_QA_B\"],\n",
        "                        help=\"The name of the task to train.\")\n",
        "    parser.add_argument(\"--data_dir\",\n",
        "                        default=None,\n",
        "                        type=str,\n",
        "                        required=True,\n",
        "                        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
        "    parser.add_argument(\"--vocab_file\",\n",
        "                        default=None,\n",
        "                        type=str,\n",
        "                        required=True,\n",
        "                        help=\"The vocabulary file that the BERT model was trained on.\")\n",
        "    parser.add_argument(\"--bert_config_file\",\n",
        "                        default=None,\n",
        "                        type=str,\n",
        "                        required=True,\n",
        "                        help=\"The config json file corresponding to the pre-trained BERT model. \\n\"\n",
        "                             \"This specifies the model architecture.\")\n",
        "    parser.add_argument(\"--output_dir\",\n",
        "                        default=None,\n",
        "                        type=str,\n",
        "                        required=True,\n",
        "                        help=\"The output directory where the model checkpoints will be written.\")\n",
        "    parser.add_argument(\"--init_checkpoint\",\n",
        "                        default=None,\n",
        "                        type=str,\n",
        "                        required=True,\n",
        "                        help=\"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
        "    \n",
        "    ## Other parameters\n",
        "    parser.add_argument(\"--eval_test\",\n",
        "                        default=False,\n",
        "                        action='store_true',\n",
        "                        help=\"Whether to run eval on the test set.\")                    \n",
        "    parser.add_argument(\"--do_lower_case\",\n",
        "                        default=False,\n",
        "                        action='store_true',\n",
        "                        help=\"Whether to lower case the input text. True for uncased models, False for cased models.\")\n",
        "    parser.add_argument(\"--max_seq_length\",\n",
        "                        default=128,\n",
        "                        type=int,\n",
        "                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
        "                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
        "                             \"than this will be padded.\")\n",
        "    parser.add_argument(\"--train_batch_size\",\n",
        "                        default=32,\n",
        "                        type=int,\n",
        "                        help=\"Total batch size for training.\")\n",
        "    parser.add_argument(\"--eval_batch_size\",\n",
        "                        default=8,\n",
        "                        type=int,\n",
        "                        help=\"Total batch size for eval.\")\n",
        "    parser.add_argument(\"--learning_rate\",\n",
        "                        default=5e-5,\n",
        "                        type=float,\n",
        "                        help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--num_train_epochs\",\n",
        "                        default=3.0,\n",
        "                        type=float,\n",
        "                        help=\"Total number of training epochs to perform.\")\n",
        "    parser.add_argument(\"--warmup_proportion\",\n",
        "                        default=0.1,\n",
        "                        type=float,\n",
        "                        help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
        "                             \"E.g., 0.1 = 10%% of training.\")\n",
        "    parser.add_argument(\"--no_cuda\",\n",
        "                        default=False,\n",
        "                        action='store_true',\n",
        "                        help=\"Whether not to use CUDA when available\")\n",
        "    parser.add_argument(\"--accumulate_gradients\",\n",
        "                        type=int,\n",
        "                        default=1,\n",
        "                        help=\"Number of steps to accumulate gradient on (divide the batch_size and accumulate)\")\n",
        "    parser.add_argument(\"--local_rank\",\n",
        "                        type=int,\n",
        "                        default=-1,\n",
        "                        help=\"local_rank for distributed training on gpus\")\n",
        "    parser.add_argument('--seed', \n",
        "                        type=int, \n",
        "                        default=42,\n",
        "                        help=\"random seed for initialization\")\n",
        "    parser.add_argument('--gradient_accumulation_steps',\n",
        "                        type=int,\n",
        "                        default=1,\n",
        "                        help=\"Number of updates steps to accumualte before performing a backward/update pass.\")                       \n",
        "    args = parser.parse_args(args=['--task_name', 'sentihood_NLI_M', \n",
        "    \n",
        "'--data_dir', '/content/drive/My Drive/My Data/data/sentihood/bert-pair/',\n",
        "'--vocab_file', '/content/drive/My Drive/My Data/uncased_L-12_H-768_A-12/vocab.txt',\n",
        "'--bert_config_file', '/content/drive/My Drive/My Data/uncased_L-12_H-768_A-12/bert_config.json', \n",
        "'--init_checkpoint', '/content/drive/My Drive/My Data/uncased_L-12_H-768_A-12/pytorch_model.bin', \n",
        "'--eval_test', \n",
        "'--do_lower_case', \n",
        "'--max_seq_length', '512', \n",
        "'--train_batch_size', '24', \n",
        "'--learning_rate', '2e-5', \n",
        "'--num_train_epochs', '6.0', \n",
        "'--output_dir', '/content/drive/My Drive/My Data/results/sentihood/NLI_M',\n",
        "'--seed', '42'])\n",
        "\n",
        "\n",
        "    if args.local_rank == -1 or args.no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        n_gpu = torch.cuda.device_count()\n",
        "    else:\n",
        "        device = torch.device(\"cuda\", args.local_rank)\n",
        "        n_gpu = 1\n",
        "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "    logger.info(\"device %s n_gpu %d distributed training %r\", device, n_gpu, bool(args.local_rank != -1))\n",
        "\n",
        "    if args.accumulate_gradients < 1:\n",
        "        raise ValueError(\"Invalid accumulate_gradients parameter: {}, should be >= 1\".format(\n",
        "                            args.accumulate_gradients))\n",
        "\n",
        "    args.train_batch_size = int(args.train_batch_size / args.accumulate_gradients)\n",
        "\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "    bert_config = BertConfig.from_json_file(args.bert_config_file)\n",
        "\n",
        "    if args.max_seq_length > bert_config.max_position_embeddings:\n",
        "        raise ValueError(\n",
        "            \"Cannot use sequence length {} because the BERT model was only trained up to sequence length {}\".format(\n",
        "            args.max_seq_length, bert_config.max_position_embeddings))\n",
        "\n",
        "    if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n",
        "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
        "    os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    # prepare dataloaders\n",
        "    processors = {\n",
        "        \"sentihood_single\":Sentihood_single_Processor,\n",
        "        \"sentihood_NLI_M\":Sentihood_NLI_M_Processor,\n",
        "        \"sentihood_QA_M\":Sentihood_QA_M_Processor,\n",
        "        \"sentihood_NLI_B\":Sentihood_NLI_B_Processor,\n",
        "        \"sentihood_QA_B\":Sentihood_QA_B_Processor,\n",
        "        \"semeval_single\":Semeval_single_Processor,\n",
        "        \"semeval_NLI_M\":Semeval_NLI_M_Processor,\n",
        "        \"semeval_QA_M\":Semeval_QA_M_Processor,\n",
        "        \"semeval_NLI_B\":Semeval_NLI_B_Processor,\n",
        "        \"semeval_QA_B\":Semeval_QA_B_Processor,\n",
        "    }\n",
        "\n",
        "    processor = processors[args.task_name]()\n",
        "    label_list = processor.get_labels()\n",
        "\n",
        "    tokenizer = FullTokenizer(\n",
        "        vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)\n",
        "\n",
        "    # training set\n",
        "    train_examples = None\n",
        "    num_train_steps = None\n",
        "    train_examples = processor.get_train_examples(args.data_dir)\n",
        "    num_train_steps = int(\n",
        "        len(train_examples) / args.train_batch_size * args.num_train_epochs)\n",
        "\n",
        "    train_features = convert_examples_to_features(\n",
        "        train_examples, label_list, args.max_seq_length, tokenizer)\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
        "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
        "    logger.info(\"  Num steps = %d\", num_train_steps)\n",
        "\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
        "    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
        "\n",
        "    train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "    if args.local_rank == -1:\n",
        "        train_sampler = RandomSampler(train_data)\n",
        "    else:\n",
        "        train_sampler = DistributedSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "    # test set\n",
        "    if args.eval_test:\n",
        "        test_examples = processor.get_test_examples(args.data_dir)\n",
        "        test_features = convert_examples_to_features(\n",
        "            test_examples, label_list, args.max_seq_length, tokenizer)\n",
        "\n",
        "        all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
        "        all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
        "        all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
        "        all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)\n",
        "\n",
        "        test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "        test_dataloader = DataLoader(test_data, batch_size=args.eval_batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "    # model and optimizer\n",
        "    model = BertForSequenceClassification(bert_config, len(label_list))\n",
        "    if args.init_checkpoint is not None:\n",
        "        model.bert.load_state_dict(torch.load(args.init_checkpoint, map_location='cpu'))\n",
        "    model.to(device)\n",
        "\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
        "                                                          output_device=args.local_rank)\n",
        "    elif n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_parameters = [\n",
        "         {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
        "         {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
        "         ]\n",
        "\t\t\n",
        "    optimizer = BERTAdam(optimizer_parameters,\n",
        "                         lr=args.learning_rate,\n",
        "                         warmup=args.warmup_proportion,\n",
        "                         t_total=num_train_steps)\n",
        "\n",
        "\n",
        "    # train\n",
        "    output_log_file = os.path.join(args.output_dir, \"log.txt\")\n",
        "    print(\"output_log_file=\",output_log_file)\n",
        "    with open(output_log_file, \"w\") as writer:\n",
        "        if args.eval_test:\n",
        "            writer.write(\"epoch\\tglobal_step\\tloss\\ttest_loss\\ttest_accuracy\\n\")\n",
        "        else:\n",
        "            writer.write(\"epoch\\tglobal_step\\tloss\\n\")\n",
        "    \n",
        "    global_step = 0\n",
        "    epoch=0\n",
        "    for _ in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
        "        epoch+=1\n",
        "        model.train()\n",
        "        tr_loss = 0\n",
        "        nb_tr_examples, nb_tr_steps = 0, 0\n",
        "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, input_mask, segment_ids, label_ids = batch\n",
        "            loss, _ = model(input_ids, segment_ids, input_mask, label_ids)\n",
        "            if n_gpu > 1:\n",
        "                loss = loss.mean() # mean() to average on multi-gpu.\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "            tr_loss += loss.item()\n",
        "            nb_tr_examples += input_ids.size(0)\n",
        "            nb_tr_steps += 1\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                optimizer.step()    # We have accumulated enought gradients\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "        \n",
        "        # eval_test\n",
        "        if args.eval_test:\n",
        "            model.eval()\n",
        "            test_loss, test_accuracy = 0, 0\n",
        "            nb_test_steps, nb_test_examples = 0, 0\n",
        "            with open(os.path.join(args.output_dir, \"test_ep_\"+str(epoch)+\".txt\"),\"w\") as f_test:\n",
        "                for input_ids, input_mask, segment_ids, label_ids in test_dataloader:\n",
        "                    input_ids = input_ids.to(device)\n",
        "                    input_mask = input_mask.to(device)\n",
        "                    segment_ids = segment_ids.to(device)\n",
        "                    label_ids = label_ids.to(device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        tmp_test_loss, logits = model(input_ids, segment_ids, input_mask, label_ids)\n",
        "\n",
        "                    logits = F.softmax(logits, dim=-1)\n",
        "                    logits = logits.detach().cpu().numpy()\n",
        "                    label_ids = label_ids.to('cpu').numpy()\n",
        "                    outputs = np.argmax(logits, axis=1)\n",
        "                    for output_i in range(len(outputs)):\n",
        "                        f_test.write(str(outputs[output_i]))\n",
        "                        for ou in logits[output_i]:\n",
        "                            f_test.write(\" \"+str(ou))\n",
        "                        f_test.write(\"\\n\")\n",
        "                    tmp_test_accuracy=np.sum(outputs == label_ids)\n",
        "\n",
        "                    test_loss += tmp_test_loss.mean().item()\n",
        "                    test_accuracy += tmp_test_accuracy\n",
        "\n",
        "                    nb_test_examples += input_ids.size(0)\n",
        "                    nb_test_steps += 1\n",
        "\n",
        "            test_loss = test_loss / nb_test_steps\n",
        "            test_accuracy = test_accuracy / nb_test_examples\n",
        "\n",
        "\n",
        "        result = collections.OrderedDict()\n",
        "        if args.eval_test:\n",
        "            result = {'epoch': epoch,\n",
        "                    'global_step': global_step,\n",
        "                    'loss': tr_loss/nb_tr_steps,\n",
        "                    'test_loss': test_loss,\n",
        "                    'test_accuracy': test_accuracy}\n",
        "        else:\n",
        "            result = {'epoch': epoch,\n",
        "                    'global_step': global_step,\n",
        "                    'loss': tr_loss/nb_tr_steps}\n",
        "\n",
        "        logger.info(\"***** Eval results *****\")\n",
        "        with open(output_log_file, \"a+\") as writer:\n",
        "            for key in result.keys():\n",
        "                logger.info(\"  %s = %s\\n\", key, str(result[key]))\n",
        "                writer.write(\"%s\\t\" % (str(result[key])))\n",
        "            writer.write(\"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/28/2020 01:51:00 - INFO - __main__ -   device cpu n_gpu 0 distributed training False\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-f672136b36aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-106-f672136b36aa>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0mtrain_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0mnum_train_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0mtrain_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     num_train_steps = int(\n\u001b[1;32m    319\u001b[0m         len(train_examples) / args.train_batch_size * args.num_train_epochs)\n",
            "\u001b[0;32m<ipython-input-104-fe9f2987891a>\u001b[0m in \u001b[0;36mget_train_examples\u001b[0;34m(self, data_dir)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_train_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;34m\"\"\"See base class.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/drive/My Drive/My Data/data/sentihood/bert-pair/train_NLI_M.tsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File /content/drive/My Drive/My Data/data/sentihood/bert-pair/train_NLI_M.tsv does not exist: '/content/drive/My Drive/My Data/data/sentihood/bert-pair/train_NLI_M.tsv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Py98BlzlqoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "09962ca2-9c7b-403d-f8b9-370be27ac983"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88bAIlXm_9O1",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxbPPnZRbOnm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "fa123aba-3e78-4bb1-ef55-b36609b78b75"
      },
      "source": [
        "import argparse\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "\n",
        "def get_y_true(task_name):\n",
        "    \"\"\" \n",
        "    Read file to obtain y_true.\n",
        "    All of five tasks of Sentihood use the test set of task-BERT-pair-NLI-M to get true labels.\n",
        "    All of five tasks of SemEval-2014 use the test set of task-BERT-pair-NLI-M to get true labels.\n",
        "    \"\"\"\n",
        "    if task_name in [\"sentihood_single\", \"sentihood_NLI_M\", \"sentihood_QA_M\", \"sentihood_NLI_B\", \"sentihood_QA_B\"]:\n",
        "        true_data_file = \"/content/drive/My Drive/My Data/data/sentihood/bert-pair/test_NLI_M.tsv\"\n",
        "\n",
        "        df = pd.read_csv(true_data_file,sep='\\t')\n",
        "        y_true = []\n",
        "        for i in range(len(df)):\n",
        "            label = df['label'][i]\n",
        "            assert label in ['None', 'Positive', 'Negative'], \"error!\"\n",
        "            if label == 'None':\n",
        "                n = 0\n",
        "            elif label == 'Positive':\n",
        "                n = 1\n",
        "            else:\n",
        "                n = 2\n",
        "            y_true.append(n)\n",
        "    else:\n",
        "        true_data_file = \"/content/drive/My Drive/My Data/data/semeval2014/bert-pair/test_NLI_M.csv\"\n",
        "\n",
        "        df = pd.read_csv(true_data_file,sep='\\t',header=None).values\n",
        "        y_true=[]\n",
        "        for i in range(len(df)):\n",
        "            label = df[i][1]\n",
        "            assert label in ['positive', 'neutral', 'negative', 'conflict', 'none'], \"error!\"\n",
        "            if label == 'positive':\n",
        "                n = 0\n",
        "            elif label == 'neutral':\n",
        "                n = 1\n",
        "            elif label == 'negative':\n",
        "                n = 2\n",
        "            elif label == 'conflict':\n",
        "                n = 3\n",
        "            elif label == 'none':\n",
        "                n = 4\n",
        "            y_true.append(n)\n",
        "    \n",
        "    return y_true\n",
        "\n",
        "\n",
        "def get_y_pred(task_name, pred_data_dir):\n",
        "    \"\"\" \n",
        "    Read file to obtain y_pred and scores.\n",
        "    \"\"\"\n",
        "    pred=[]\n",
        "    score=[]\n",
        "    if task_name in [\"sentihood_NLI_M\", \"sentihood_QA_M\"]:\n",
        "        with open(pred_data_dir, \"r\", encoding=\"utf-8\") as f:\n",
        "            s=f.readline().strip().split()\n",
        "            while s:\n",
        "                pred.append(int(s[0]))\n",
        "                score.append([float(s[1]),float(s[2]),float(s[3])])\n",
        "                s = f.readline().strip().split()\n",
        "    elif task_name in [\"sentihood_NLI_B\", \"sentihood_QA_B\"]:\n",
        "        count = 0\n",
        "        tmp = []\n",
        "        with open(pred_data_dir, \"r\", encoding=\"utf-8\") as f:\n",
        "            s = f.readline().strip().split()\n",
        "            while s:\n",
        "                tmp.append([float(s[2])])\n",
        "                count += 1\n",
        "                if count % 3 == 0:\n",
        "                    tmp_sum = np.sum(tmp)\n",
        "                    t = []\n",
        "                    for i in range(3):\n",
        "                        t.append(tmp[i] / tmp_sum)\n",
        "                    score.append(t)\n",
        "                    if t[0] >= t[1] and t[0] >= t[2]:\n",
        "                        pred.append(0)\n",
        "                    elif t[1] >= t[0] and t[1] >= t[2]:\n",
        "                        pred.append(1)\n",
        "                    else:\n",
        "                        pred.append(2)\n",
        "                    tmp = []\n",
        "                s = f.readline().strip().split()\n",
        "    elif task_name == \"sentihood_single\":\n",
        "        count = 0\n",
        "        with open(pred_data_dir + \"loc1_general.txt\", \"r\", encoding=\"utf-8\") as f1_general, \\\n",
        "            open(pred_data_dir + \"loc1_price.txt\", \"r\", encoding=\"utf-8\") as f1_price, \\\n",
        "            open(pred_data_dir + \"loc1_safety.txt\", \"r\", encoding=\"utf-8\") as f1_safety, \\\n",
        "            open(pred_data_dir + \"loc1_transit.txt\", \"r\", encoding=\"utf-8\") as f1_transit:\n",
        "            s = f1_general.readline().strip().split()\n",
        "            while s:\n",
        "                count += 1\n",
        "                pred.append(int(s[0]))\n",
        "                score.append([float(s[1]), float(s[2]), float(s[3])])\n",
        "                if count % 4 == 0:\n",
        "                    s = f1_general.readline().strip().split()\n",
        "                if count % 4 == 1:\n",
        "                    s = f1_price.readline().strip().split()\n",
        "                if count % 4 == 2:\n",
        "                    s = f1_safety.readline().strip().split()\n",
        "                if count % 4 == 3:\n",
        "                    s = f1_transit.readline().strip().split()\n",
        "\n",
        "        with open(pred_data_dir + \"loc2_general.txt\", \"r\", encoding=\"utf-8\") as f2_general, \\\n",
        "            open(pred_data_dir + \"loc2_price.txt\", \"r\", encoding=\"utf-8\") as f2_price, \\\n",
        "            open(pred_data_dir + \"loc2_safety.txt\", \"r\", encoding=\"utf-8\") as f2_safety, \\\n",
        "            open(pred_data_dir + \"loc2_transit.txt\", \"r\", encoding=\"utf-8\") as f2_transit:\n",
        "            s = f2_general.readline().strip().split()\n",
        "            while s:\n",
        "                count += 1\n",
        "                pred.append(int(s[0]))\n",
        "                score.append([float(s[1]), float(s[2]), float(s[3])])\n",
        "                if count % 4 == 0:\n",
        "                    s = f2_general.readline().strip().split()\n",
        "                if count % 4 == 1:\n",
        "                    s = f2_price.readline().strip().split()\n",
        "                if count % 4 == 2:\n",
        "                    s = f2_safety.readline().strip().split()\n",
        "                if count % 4 == 3:\n",
        "                    s = f2_transit.readline().strip().split()\n",
        "    elif task_name in [\"semeval_NLI_M\", \"semeval_QA_M\"]:\n",
        "        with open(pred_data_dir,\"r\",encoding=\"utf-8\") as f:\n",
        "            s=f.readline().strip().split()\n",
        "            while s:\n",
        "                pred.append(int(s[0]))\n",
        "                score.append([float(s[1]), float(s[2]), float(s[3]), float(s[4]), float(s[5])])\n",
        "                s = f.readline().strip().split()\n",
        "    elif task_name in [\"semeval_NLI_B\", \"semeval_QA_B\"]:\n",
        "        count = 0\n",
        "        tmp = []\n",
        "        with open(pred_data_dir, \"r\", encoding=\"utf-8\") as f:\n",
        "            s = f.readline().strip().split()\n",
        "            while s:\n",
        "                tmp.append([float(s[2])])\n",
        "                count += 1\n",
        "                if count % 5 == 0:\n",
        "                    tmp_sum = np.sum(tmp)\n",
        "                    t = []\n",
        "                    for i in range(5):\n",
        "                        t.append(tmp[i] / tmp_sum)\n",
        "                    score.append(t)\n",
        "                    if t[0] >= t[1] and t[0] >= t[2] and t[0]>=t[3] and t[0]>=t[4]:\n",
        "                        pred.append(0)\n",
        "                    elif t[1] >= t[0] and t[1] >= t[2] and t[1]>=t[3] and t[1]>=t[4]:\n",
        "                        pred.append(1)\n",
        "                    elif t[2] >= t[0] and t[2] >= t[1] and t[2]>=t[3] and t[2]>=t[4]:\n",
        "                        pred.append(2)\n",
        "                    elif t[3] >= t[0] and t[3] >= t[1] and t[3]>=t[2] and t[3]>=t[4]:\n",
        "                        pred.append(3)\n",
        "                    else:\n",
        "                        pred.append(4)\n",
        "                    tmp = []\n",
        "                s = f.readline().strip().split()\n",
        "    else: \n",
        "        count = 0\n",
        "        with open(pred_data_dir+\"price.txt\",\"r\",encoding=\"utf-8\") as f_price, \\\n",
        "            open(pred_data_dir+\"anecdotes.txt\", \"r\", encoding=\"utf-8\") as f_anecdotes, \\\n",
        "            open(pred_data_dir+\"food.txt\", \"r\", encoding=\"utf-8\") as f_food, \\\n",
        "            open(pred_data_dir+\"ambience.txt\", \"r\", encoding=\"utf-8\") as f_ambience, \\\n",
        "            open(pred_data_dir+\"service.txt\", \"r\", encoding=\"utf-8\") as f_service:\n",
        "            s = f_price.readline().strip().split()\n",
        "            while s:\n",
        "                count += 1\n",
        "                pred.append(int(s[0]))\n",
        "                score.append([float(s[1]), float(s[2]), float(s[3]), float(s[4]), float(s[5])])\n",
        "                if count % 5 == 0:\n",
        "                    s = f_price.readline().strip().split()\n",
        "                if count % 5 == 1:\n",
        "                    s = f_anecdotes.readline().strip().split()\n",
        "                if count % 5 == 2:\n",
        "                    s = f_food.readline().strip().split()\n",
        "                if count % 5 == 3:\n",
        "                    s = f_ambience.readline().strip().split()\n",
        "                if count % 5 == 4:\n",
        "                    s = f_service.readline().strip().split()\n",
        "\n",
        "    return pred, score\n",
        "\n",
        "\n",
        "def sentihood_strict_acc(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate \"strict Acc\" of aspect detection task of Sentihood.\n",
        "    \"\"\"\n",
        "    total_cases=int(len(y_true)/4)\n",
        "    true_cases=0\n",
        "    for i in range(total_cases):\n",
        "        if y_true[i*4]!=y_pred[i*4]:continue\n",
        "        if y_true[i*4+1]!=y_pred[i*4+1]:continue\n",
        "        if y_true[i*4+2]!=y_pred[i*4+2]:continue\n",
        "        if y_true[i*4+3]!=y_pred[i*4+3]:continue\n",
        "        true_cases+=1\n",
        "    aspect_strict_Acc = true_cases/total_cases\n",
        "\n",
        "    return aspect_strict_Acc\n",
        "\n",
        "\n",
        "def sentihood_macro_F1(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate \"Macro-F1\" of aspect detection task of Sentihood.\n",
        "    \"\"\"\n",
        "    p_all=0\n",
        "    r_all=0\n",
        "    count=0\n",
        "    for i in range(len(y_pred)//4):\n",
        "        a=set()\n",
        "        b=set()\n",
        "        for j in range(4):\n",
        "            if y_pred[i*4+j]!=0:\n",
        "                a.add(j)\n",
        "            if y_true[i*4+j]!=0:\n",
        "                b.add(j)\n",
        "        if len(b)==0:continue\n",
        "        a_b=a.intersection(b)\n",
        "        if len(a_b)>0:\n",
        "            p=len(a_b)/len(a)\n",
        "            r=len(a_b)/len(b)\n",
        "        else:\n",
        "            p=0\n",
        "            r=0\n",
        "        count+=1\n",
        "        p_all+=p\n",
        "        r_all+=r\n",
        "    Ma_p=p_all/count\n",
        "    Ma_r=r_all/count\n",
        "    aspect_Macro_F1 = 2*Ma_p*Ma_r/(Ma_p+Ma_r)\n",
        "\n",
        "    return aspect_Macro_F1\n",
        "\n",
        "\n",
        "def sentihood_AUC_Acc(y_true, score):\n",
        "    \"\"\"\n",
        "    Calculate \"Macro-AUC\" of both aspect detection and sentiment classification tasks of Sentihood.\n",
        "    Calculate \"Acc\" of sentiment classification task of Sentihood.\n",
        "    \"\"\"\n",
        "    # aspect-Macro-AUC\n",
        "    aspect_y_true=[]\n",
        "    aspect_y_score=[]\n",
        "    aspect_y_trues=[[],[],[],[]]\n",
        "    aspect_y_scores=[[],[],[],[]]\n",
        "    for i in range(len(y_true)):\n",
        "        if y_true[i]>0:\n",
        "            aspect_y_true.append(0)\n",
        "        else:\n",
        "            aspect_y_true.append(1) # \"None\": 1\n",
        "        tmp_score=score[i][0] # probability of \"None\"\n",
        "        aspect_y_score.append(tmp_score)\n",
        "        aspect_y_trues[i%4].append(aspect_y_true[-1])\n",
        "        aspect_y_scores[i%4].append(aspect_y_score[-1])\n",
        "\n",
        "    aspect_auc=[]\n",
        "    for i in range(4):\n",
        "        aspect_auc.append(metrics.roc_auc_score(aspect_y_trues[i], aspect_y_scores[i]))\n",
        "    aspect_Macro_AUC = np.mean(aspect_auc)\n",
        "    \n",
        "    # sentiment-Macro-AUC\n",
        "    sentiment_y_true=[]\n",
        "    sentiment_y_pred=[]\n",
        "    sentiment_y_score=[]\n",
        "    sentiment_y_trues=[[],[],[],[]]\n",
        "    sentiment_y_scores=[[],[],[],[]]\n",
        "    for i in range(len(y_true)):\n",
        "        if y_true[i]>0:\n",
        "            sentiment_y_true.append(y_true[i]-1) # \"Postive\":0, \"Negative\":1\n",
        "            tmp_score=score[i][2]/(score[i][1]+score[i][2])  # probability of \"Negative\"\n",
        "            sentiment_y_score.append(tmp_score)\n",
        "            if tmp_score>0.5:\n",
        "                sentiment_y_pred.append(1) # \"Negative\": 1\n",
        "            else:\n",
        "                sentiment_y_pred.append(0)\n",
        "            sentiment_y_trues[i%4].append(sentiment_y_true[-1])\n",
        "            sentiment_y_scores[i%4].append(sentiment_y_score[-1])\n",
        "\n",
        "    sentiment_auc=[]\n",
        "    for i in range(4):\n",
        "        sentiment_auc.append(metrics.roc_auc_score(sentiment_y_trues[i], sentiment_y_scores[i]))\n",
        "    sentiment_Macro_AUC = np.mean(sentiment_auc)\n",
        "\n",
        "    # sentiment Acc\n",
        "    sentiment_y_true = np.array(sentiment_y_true)\n",
        "    sentiment_y_pred = np.array(sentiment_y_pred)\n",
        "    sentiment_Acc = metrics.accuracy_score(sentiment_y_true,sentiment_y_pred)\n",
        "\n",
        "    return aspect_Macro_AUC, sentiment_Acc, sentiment_Macro_AUC\n",
        "\n",
        "\n",
        "def semeval_PRF(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate \"Micro P R F\" of aspect detection task of SemEval-2014.\n",
        "    \"\"\"\n",
        "    s_all=0\n",
        "    g_all=0\n",
        "    s_g_all=0\n",
        "    for i in range(len(y_pred)//5):\n",
        "        s=set()\n",
        "        g=set()\n",
        "        for j in range(5):\n",
        "            if y_pred[i*5+j]!=4:\n",
        "                s.add(j)\n",
        "            if y_true[i*5+j]!=4:\n",
        "                g.add(j)\n",
        "        if len(g)==0:continue\n",
        "        s_g=s.intersection(g)\n",
        "        s_all+=len(s)\n",
        "        g_all+=len(g)\n",
        "        s_g_all+=len(s_g)\n",
        "\n",
        "    p=s_g_all/s_all\n",
        "    r=s_g_all/g_all\n",
        "    f=2*p*r/(p+r)\n",
        "\n",
        "    return p,r,f\n",
        "\n",
        "\n",
        "def semeval_Acc(y_true, y_pred, score, classes=4):\n",
        "    \"\"\"\n",
        "    Calculate \"Acc\" of sentiment classification task of SemEval-2014.\n",
        "    \"\"\"\n",
        "    assert classes in [2, 3, 4], \"classes must be 2 or 3 or 4.\"\n",
        "\n",
        "    if classes == 4:\n",
        "        total=0\n",
        "        total_right=0\n",
        "        for i in range(len(y_true)):\n",
        "            if y_true[i]==4:continue\n",
        "            total+=1\n",
        "            tmp=y_pred[i]\n",
        "            if tmp==4:\n",
        "                if score[i][0]>=score[i][1] and score[i][0]>=score[i][2] and score[i][0]>=score[i][3]:\n",
        "                    tmp=0\n",
        "                elif score[i][1]>=score[i][0] and score[i][1]>=score[i][2] and score[i][1]>=score[i][3]:\n",
        "                    tmp=1\n",
        "                elif score[i][2]>=score[i][0] and score[i][2]>=score[i][1] and score[i][2]>=score[i][3]:\n",
        "                    tmp=2\n",
        "                else:\n",
        "                    tmp=3\n",
        "            if y_true[i]==tmp:\n",
        "                total_right+=1\n",
        "        sentiment_Acc = total_right/total\n",
        "    elif classes == 3:\n",
        "        total=0\n",
        "        total_right=0\n",
        "        for i in range(len(y_true)):\n",
        "            if y_true[i]>=3:continue\n",
        "            total+=1\n",
        "            tmp=y_pred[i]\n",
        "            if tmp>=3:\n",
        "                if score[i][0]>=score[i][1] and score[i][0]>=score[i][2]:\n",
        "                    tmp=0\n",
        "                elif score[i][1]>=score[i][0] and score[i][1]>=score[i][2]:\n",
        "                    tmp=1\n",
        "                else:\n",
        "                    tmp=2\n",
        "            if y_true[i]==tmp:\n",
        "                total_right+=1\n",
        "        sentiment_Acc = total_right/total\n",
        "    else:\n",
        "        total=0\n",
        "        total_right=0\n",
        "        for i in range(len(y_true)):\n",
        "            if y_true[i]>=3 or y_true[i]==1:continue\n",
        "            total+=1\n",
        "            tmp=y_pred[i]\n",
        "            if tmp>=3 or tmp==1:\n",
        "                if score[i][0]>=score[i][2]:\n",
        "                    tmp=0\n",
        "                else:\n",
        "                    tmp=2\n",
        "            if y_true[i]==tmp:\n",
        "                total_right+=1\n",
        "        sentiment_Acc = total_right/total\n",
        "\n",
        "    return sentiment_Acc\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--task_name\",\n",
        "                        default=None,\n",
        "                        type=str,\n",
        "                        required=True,\n",
        "                        choices=[\"sentihood_single\", \"sentihood_NLI_M\", \"sentihood_QA_M\", \\\n",
        "                                \"sentihood_NLI_B\", \"sentihood_QA_B\", \"semeval_single\", \\\n",
        "                                \"semeval_NLI_M\", \"semeval_QA_M\", \"semeval_NLI_B\", \"semeval_QA_B\"],\n",
        "                        help=\"The name of the task to evalution.\")\n",
        "    parser.add_argument(\"--pred_data_dir\",\n",
        "                        default=None,\n",
        "                        type=str,\n",
        "                        required=True,\n",
        "                        help=\"The pred data dir.\")\n",
        "    args = parser.parse_args(args=[\n",
        "'--task_name', 'sentihood_NLI_M', \n",
        "'--pred_data_dir', '/content/drive/My Drive/My Data/results/sentihood/NLI_M/test_ep_4.txt'])\n",
        "\n",
        "    result = collections.OrderedDict()\n",
        "    if args.task_name in [\"sentihood_single\", \"sentihood_NLI_M\", \"sentihood_QA_M\", \"sentihood_NLI_B\", \"sentihood_QA_B\"]:\n",
        "        y_true = get_y_true(args.task_name)\n",
        "        y_pred, score = get_y_pred(args.task_name, args.pred_data_dir)\n",
        "        aspect_strict_Acc = sentihood_strict_acc(y_true, y_pred)\n",
        "        aspect_Macro_F1 = sentihood_macro_F1(y_true, y_pred)\n",
        "        aspect_Macro_AUC, sentiment_Acc, sentiment_Macro_AUC = sentihood_AUC_Acc(y_true, score)\n",
        "        result = {'aspect_strict_Acc': aspect_strict_Acc,\n",
        "                'aspect_Macro_F1': aspect_Macro_F1,\n",
        "                'aspect_Macro_AUC': aspect_Macro_AUC,\n",
        "                'sentiment_Acc': sentiment_Acc,\n",
        "                'sentiment_Macro_AUC': sentiment_Macro_AUC}\n",
        "    else:\n",
        "        y_true = get_y_true(args.task_name)\n",
        "        y_pred, score = get_y_pred(args.task_name, args.pred_data_dir)\n",
        "        aspect_P, aspect_R, aspect_F = semeval_PRF(y_true, y_pred)\n",
        "        sentiment_Acc_4_classes = semeval_Acc(y_true, y_pred, score, 4)\n",
        "        sentiment_Acc_3_classes = semeval_Acc(y_true, y_pred, score, 3)\n",
        "        sentiment_Acc_2_classes = semeval_Acc(y_true, y_pred, score, 2)\n",
        "        result = {'aspect_P': aspect_P,\n",
        "                'aspect_R': aspect_R,\n",
        "                'aspect_F': aspect_F,\n",
        "                'sentiment_Acc_4_classes': sentiment_Acc_4_classes,\n",
        "                'sentiment_Acc_3_classes': sentiment_Acc_3_classes,\n",
        "                'sentiment_Acc_2_classes': sentiment_Acc_2_classes}\n",
        "\n",
        "    for key in result.keys():\n",
        "        print(key, \"=\",str(result[key]))\n",
        "    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-8963f44aa55a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-108-8963f44aa55a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"sentihood_single\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentihood_NLI_M\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentihood_QA_M\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentihood_NLI_B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentihood_QA_B\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_y_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_y_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_data_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0maspect_strict_Acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentihood_strict_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-108-8963f44aa55a>\u001b[0m in \u001b[0;36mget_y_true\u001b[0;34m(task_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtrue_data_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/My Data/data/sentihood/bert-pair/test_NLI_M.tsv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_data_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File /content/drive/My Drive/My Data/data/sentihood/bert-pair/test_NLI_M.tsv does not exist: '/content/drive/My Drive/My Data/data/sentihood/bert-pair/test_NLI_M.tsv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ipoynE-KCE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LGH4dMnAAJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}